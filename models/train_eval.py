#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ¨¡å—

æä¾›åŸºäºLightGBMçš„åºæ•°åˆ†ç±»æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½
"""

import time
import logging
import numpy as np
from typing import List, Tuple, Optional, Union
from lightgbm import LGBMClassifier
from tqdm import tqdm
from .model_utils import generate_ordinal_targets, convert_ordinal_to_class, validate_predictions

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class ProgressBarCallback:
    """
    LightGBMè®­ç»ƒè¿›åº¦æ¡å›è°ƒç±»
    """
    
    def __init__(self, total: int, desc: str = "è®­ç»ƒè¿›åº¦"):
        """
        åˆå§‹åŒ–è¿›åº¦æ¡å›è°ƒ
        
        Args:
            total: æ€»çš„è®­ç»ƒè¿­ä»£æ¬¡æ•°
            desc: è¿›åº¦æ¡æè¿°æ–‡æœ¬
        """
        if total <= 0:
            raise ValueError(f"æ€»è¿­ä»£æ¬¡æ•°å¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {total}")
        
        self.total = total
        self.pbar = tqdm(
            total=total, 
            desc=desc, 
            unit="è¿­ä»£",
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        logger.debug(f"åˆå§‹åŒ–è¿›åº¦æ¡å›è°ƒ: æ€»è¿­ä»£æ¬¡æ•°={total}")

    def __call__(self, env):
        """
        å›è°ƒå‡½æ•°ï¼Œåœ¨æ¯æ¬¡è¿­ä»£æ—¶è¢«è°ƒç”¨
        """
        # æ›´æ–°è¿›åº¦æ¡
        if env.iteration < self.total:
            self.pbar.update(1)
        
        # è®­ç»ƒå®Œæˆæ—¶å…³é—­è¿›åº¦æ¡
        if env.iteration + 1 == env.end_iteration:
            self.pbar.close()
            logger.debug(f"è®­ç»ƒå®Œæˆï¼Œæ€»è¿­ä»£æ¬¡æ•°: {env.iteration + 1}")


def train_models(X_train: np.ndarray, 
                y_train_raw: np.ndarray, 
                num_classes: int = 10, 
                n_estimators: int = 1000,
                learning_rate: float = 0.05,
                num_leaves: int = 63,
                seed: int = 42,
                categorical_features: Optional[List[str]] = None,
                verbose: bool = True) -> List[LGBMClassifier]:
    """
    è®­ç»ƒå¤šä¸ªLightGBMäºŒåˆ†ç±»æ¨¡å‹ä»¥å®ç°åºæ•°åˆ†ç±»
    
    Args:
        X_train: è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µ
        y_train_raw: è®­ç»ƒé›†åŸå§‹è¯„åˆ†æ ‡ç­¾
        num_classes: æ€»ç±»åˆ«æ•°é‡
        n_estimators: æ¯ä¸ªæ¨¡å‹çš„æ ‘çš„æ•°é‡
        learning_rate: å­¦ä¹ ç‡
        num_leaves: æ¯æ£µæ ‘çš„å¶å­èŠ‚ç‚¹æ•°
        seed: éšæœºç§å­
        categorical_features: ç±»åˆ«ç‰¹å¾åˆ—è¡¨
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è®­ç»ƒä¿¡æ¯
    
    Returns:
        è®­ç»ƒå¥½çš„æ¨¡å‹åˆ—è¡¨
    """
    # ==================== å‚æ•°éªŒè¯ ====================
    if not isinstance(X_train, np.ndarray) or not isinstance(y_train_raw, np.ndarray):
        raise ValueError("è¾“å…¥çš„ç‰¹å¾å’Œæ ‡ç­¾å¿…é¡»æ˜¯numpyæ•°ç»„")
    
    if len(X_train) != len(y_train_raw):
        raise ValueError(f"ç‰¹å¾å’Œæ ‡ç­¾çš„æ ·æœ¬æ•°é‡ä¸åŒ¹é…: {len(X_train)} vs {len(y_train_raw)}")
    
    if num_classes < 2:
        raise ValueError(f"ç±»åˆ«æ•°é‡å¿…é¡»è‡³å°‘ä¸º2ï¼Œå½“å‰å€¼: {num_classes}")
    
    if n_estimators <= 0 or learning_rate <= 0 or num_leaves <= 0:
        raise ValueError("æ¨¡å‹å‚æ•°å¿…é¡»ä¸ºæ­£æ•°")
    
    # è®¾ç½®é»˜è®¤çš„ç±»åˆ«ç‰¹å¾
    if categorical_features is None:
        categorical_features = ['year_r', 'month_r', 'dayofweek_r']
    
    logger.info(f"å¼€å§‹è®­ç»ƒåºæ•°åˆ†ç±»æ¨¡å‹: æ ·æœ¬æ•°={len(X_train)}, ç‰¹å¾æ•°={X_train.shape[1]}, ç±»åˆ«æ•°={num_classes}")
    
    # ==================== ç”Ÿæˆåºæ•°ç›®æ ‡ ====================
    try:
        y_train_ordinal = generate_ordinal_targets(y_train_raw, num_classes)
        logger.info(f"åºæ•°ç›®æ ‡çŸ©é˜µç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {y_train_ordinal.shape}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆåºæ•°ç›®æ ‡æ—¶å‡ºé”™: {e}")
        raise RuntimeError(f"æ— æ³•ç”Ÿæˆåºæ•°ç›®æ ‡: {e}")
    
    # ==================== æ¨¡å‹è®­ç»ƒ ====================
    models = []
    start_time = time.time()
    
    try:
        for k in range(num_classes - 1):
            threshold_rating = (k + 1) * 0.5  # å¯¹åº”çš„è¯„åˆ†é˜ˆå€¼
            
            if verbose:
                print(f"\n{'='*60}")
                print(f"è®­ç»ƒç¬¬ {k + 1}/{num_classes - 1} ä¸ªå­æ¨¡å‹")
                print(f"ä»»åŠ¡: é¢„æµ‹è¯„åˆ†æ˜¯å¦ â‰¥ {threshold_rating:.1f} æ˜Ÿ")
                print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {y_train_ordinal[:, k].mean():.3f}")
                print(f"{'='*60}")
            
            # åˆ›å»ºå¹¶é…ç½®æ¨¡å‹
            model_k = LGBMClassifier(
                objective='binary',           # äºŒåˆ†ç±»ä»»åŠ¡
                random_state=seed,           # éšæœºç§å­
                n_estimators=n_estimators,   # æ ‘çš„æ•°é‡
                learning_rate=learning_rate, # å­¦ä¹ ç‡
                num_leaves=num_leaves,       # å¶å­èŠ‚ç‚¹æ•°
                verbosity=-1,               # é™é»˜æ¨¡å¼
                n_jobs=1,                   # ä½¿ç”¨å•çº¿ç¨‹é¿å…æ®µé”™è¯¯
                importance_type='gain'       # ç‰¹å¾é‡è¦æ€§è®¡ç®—æ–¹å¼
            )
            
            # è®­ç»ƒæ¨¡å‹
            logger.debug(f"å¼€å§‹è®­ç»ƒç¬¬{k+1}ä¸ªæ¨¡å‹ï¼Œç›®æ ‡å˜é‡ç»Ÿè®¡: {np.bincount(y_train_ordinal[:, k])}")
            
            model_k.fit(
                X_train, 
                y_train_ordinal[:, k],
                categorical_feature=categorical_features
            )
            
            models.append(model_k)
            
            # è®°å½•è®­ç»ƒä¿¡æ¯
            train_score = model_k.score(X_train, y_train_ordinal[:, k])
            logger.info(f"ç¬¬{k+1}ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè®­ç»ƒå‡†ç¡®ç‡: {train_score:.4f}")
    
    except Exception as e:
        logger.error(f"æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise RuntimeError(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
    
    # ==================== è®­ç»ƒå®Œæˆ ====================
    end_time = time.time()
    total_time = end_time - start_time
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"ğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»ç”¨æ—¶: {total_time:.2f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
        print(f"å¹³å‡æ¯ä¸ªæ¨¡å‹: {total_time/(num_classes-1):.2f} ç§’")
        print(f"è®­ç»ƒäº† {len(models)} ä¸ªäºŒåˆ†ç±»å™¨")
        print(f"{'='*60}")
    
    logger.info(f"åºæ•°åˆ†ç±»æ¨¡å‹è®­ç»ƒå®Œæˆ: ç”¨æ—¶{total_time:.2f}ç§’, æ¨¡å‹æ•°é‡{len(models)}")
    
    return models


def predict(models: List[LGBMClassifier], 
           X_val: np.ndarray,
           threshold: float = 0.5,
           return_probabilities: bool = False,
           verbose: bool = True) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„åºæ•°åˆ†ç±»æ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    è¯¥å‡½æ•°ä½¿ç”¨è®­ç»ƒå¥½çš„å¤šä¸ªäºŒåˆ†ç±»å™¨è¿›è¡Œåºæ•°åˆ†ç±»é¢„æµ‹ã€‚
    é¢„æµ‹è¿‡ç¨‹åŒ…æ‹¬ï¼š
    1. ä½¿ç”¨æ¯ä¸ªäºŒåˆ†ç±»å™¨é¢„æµ‹æ¦‚ç‡
    2. æ ¹æ®é˜ˆå€¼å°†æ¦‚ç‡è½¬æ¢ä¸ºäºŒåˆ†ç±»ç»“æœ
    3. ç»Ÿè®¡è¶…è¿‡é˜ˆå€¼çš„åˆ†ç±»å™¨æ•°é‡å¾—åˆ°æœ€ç»ˆç±»åˆ«
    
    Args:
        models (List[LGBMClassifier]): è®­ç»ƒå¥½çš„æ¨¡å‹åˆ—è¡¨
        X_val (np.ndarray): éªŒè¯é›†ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_samples, n_features)
        threshold (float, optional): äºŒåˆ†ç±»é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.5
        return_probabilities (bool, optional): æ˜¯å¦è¿”å›æ¦‚ç‡çŸ©é˜µï¼Œé»˜è®¤ä¸ºFalse
        verbose (bool, optional): æ˜¯å¦æ˜¾ç¤ºé¢„æµ‹ä¿¡æ¯ï¼Œé»˜è®¤ä¸ºTrue
    
    Returns:
        Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
            å¦‚æœreturn_probabilities=False: è¿”å›é¢„æµ‹çš„ç±»åˆ«æ•°ç»„
            å¦‚æœreturn_probabilities=True: è¿”å›(ç±»åˆ«æ•°ç»„, æ¦‚ç‡çŸ©é˜µ)çš„å…ƒç»„
    
    Raises:
        ValueError: å½“è¾“å…¥å‚æ•°ä¸åˆæ³•æ—¶æŠ›å‡ºå¼‚å¸¸
        RuntimeError: å½“é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯æ—¶æŠ›å‡ºå¼‚å¸¸
    
    Example:
        >>> X_val = np.random.rand(100, 50)
        >>> predictions = predict(models, X_val)
        >>> print(f"é¢„æµ‹äº†{len(predictions)}ä¸ªæ ·æœ¬")
    """
    # ==================== å‚æ•°éªŒè¯ ====================
    if not models:
        raise ValueError("æ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    
    if not isinstance(X_val, np.ndarray):
        raise ValueError("éªŒè¯é›†ç‰¹å¾å¿…é¡»æ˜¯numpyæ•°ç»„")
    
    if not 0 <= threshold <= 1:
        raise ValueError(f"é˜ˆå€¼å¿…é¡»åœ¨[0, 1]èŒƒå›´å†…ï¼Œå½“å‰å€¼: {threshold}")
    
    n_samples, n_features = X_val.shape
    n_models = len(models)
    
    if verbose:
        logger.info(f"å¼€å§‹é¢„æµ‹: æ ·æœ¬æ•°={n_samples}, ç‰¹å¾æ•°={n_features}, æ¨¡å‹æ•°={n_models}")
    
    # ==================== é¢„æµ‹è¿‡ç¨‹ ====================
    try:
        # åˆå§‹åŒ–æ¦‚ç‡çŸ©é˜µ
        probabilities = np.zeros((n_samples, n_models))
        
        # ä½¿ç”¨æ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
        for k, model_k in enumerate(models):
            if verbose and k == 0:
                print(f"\nä½¿ç”¨ {n_models} ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹...")
            
            # é¢„æµ‹æ¦‚ç‡ï¼ˆå–æ­£ç±»æ¦‚ç‡ï¼‰
            prob_k = model_k.predict_proba(X_val)[:, 1]
            probabilities[:, k] = prob_k
            
            if verbose:
                logger.debug(f"æ¨¡å‹{k+1}é¢„æµ‹å®Œæˆï¼Œæ¦‚ç‡èŒƒå›´: [{prob_k.min():.3f}, {prob_k.max():.3f}]")
        
        # è½¬æ¢ä¸ºç±»åˆ«é¢„æµ‹
        class_predictions = convert_ordinal_to_class(probabilities, threshold)
        
        # éªŒè¯é¢„æµ‹ç»“æœ
        if not validate_predictions(class_predictions, expected_range=(0, n_models)):
            logger.warning("é¢„æµ‹ç»“æœå¯èƒ½å­˜åœ¨å¼‚å¸¸å€¼")
        
        if verbose:
            unique_preds, counts = np.unique(class_predictions, return_counts=True)
            logger.info(f"é¢„æµ‹å®Œæˆï¼Œç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_preds, counts))}")
            print(f"é¢„æµ‹å®Œæˆï¼é¢„æµ‹èŒƒå›´: [{class_predictions.min()}, {class_predictions.max()}]")
    
    except Exception as e:
        logger.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise RuntimeError(f"é¢„æµ‹å¤±è´¥: {e}")
    
    # ==================== è¿”å›ç»“æœ ====================
    if return_probabilities:
        return class_predictions, probabilities
    else:
        return class_predictions


def evaluate_models(models: List[LGBMClassifier], 
                   X_val: np.ndarray, 
                   y_val: np.ndarray,
                   threshold: float = 0.5) -> dict:
    """
    è¯„ä¼°åºæ•°åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½
    
    è¯¥å‡½æ•°æä¾›äº†å…¨é¢çš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€å„ç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ç­‰ã€‚
    
    Args:
        models (List[LGBMClassifier]): è®­ç»ƒå¥½çš„æ¨¡å‹åˆ—è¡¨
        X_val (np.ndarray): éªŒè¯é›†ç‰¹å¾
        y_val (np.ndarray): éªŒè¯é›†çœŸå®æ ‡ç­¾
        threshold (float, optional): é¢„æµ‹é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.5
    
    Returns:
        dict: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    
    Example:
        >>> metrics = evaluate_models(models, X_val, y_val)
        >>> print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    """
    # è¿›è¡Œé¢„æµ‹
    y_pred, probabilities = predict(models, X_val, threshold, return_probabilities=True, verbose=False)
    
    # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
    accuracy = np.mean(y_pred == y_val)
    
    # è®¡ç®—RMSEï¼ˆå°†ç±»åˆ«è½¬æ¢å›è¯„åˆ†ï¼‰
    from .model_utils import label_to_rating
    y_val_ratings = label_to_rating(y_val)
    y_pred_ratings = label_to_rating(y_pred)
    rmse = np.sqrt(np.mean((y_val_ratings - y_pred_ratings) ** 2))
    
    # è®¡ç®—MAE
    mae = np.mean(np.abs(y_val_ratings - y_pred_ratings))
    
    # æ„å»ºè¯„ä¼°ç»“æœ
    evaluation_results = {
        'accuracy': accuracy,
        'rmse': rmse,
        'mae': mae,
        'n_samples': len(y_val),
        'n_models': len(models),
        'threshold': threshold,
        'prediction_range': (y_pred.min(), y_pred.max()),
        'true_range': (y_val.min(), y_val.max())
    }
    
    logger.info(f"æ¨¡å‹è¯„ä¼°å®Œæˆ: å‡†ç¡®ç‡={accuracy:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}")
    
    return evaluation_results